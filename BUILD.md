Guia de Build - Servi√ßo de ML Conecta+Sa√∫de
Este documento descreve como construir e executar o servi√ßo de Machine Learning localmente para fins de desenvolvimento e teste.

üê≥ M√©todo 1: Execu√ß√£o com Docker (Recomendado)
A forma mais simples de rodar o projeto √© usar o arquivo docker-compose.yml que est√° na pasta raiz do projeto (Conecta-Saude-Projeto).

Navegue at√© a pasta raiz do projeto.

Execute o Docker Compose:

docker-compose up --build

O servi√ßo de ML estar√° acess√≠vel em http://localhost:8000.

üë®‚Äçüíª M√©todo 2: Execu√ß√£o Manual (Desenvolvimento Local)
Use este m√©todo para rodar o servi√ßo de forma isolada.

Pr√©-requisitos:

Python (v3.10 ou superior)

Pip (gerenciador de pacotes Python)

Passos:

Navegue at√© a pasta model-LLM:

cd model-LLM

Crie e Ative um Ambiente Virtual (Recomendado):

# Cria o ambiente virtual
python -m venv venv

# Ativa o ambiente no Windows
.\venv\Scripts\activate

# Ativa o ambiente no Linux/macOS
source venv/bin/activate

Instale as depend√™ncias:

pip install -r requirements.txt

Execute o servidor FastAPI com Uvicorn:

uvicorn app.main:app --reload

O servidor iniciar√° e ficar√° observando altera√ß√µes nos arquivos.